{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "96b629e4",
   "metadata": {},
   "source": [
    "# Jupyter file for reproduction\n",
    "\n",
    "This file shows the actual experiment results (node classification, link prediction) in our paper. It's ready to run if your output of the 'package version' snippet below matches ours (and don't forget to set your own data path):"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "110e73f7",
   "metadata": {},
   "source": [
    "## Package version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0c151eca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Package \u001B[91mnumpy\u001B[00m, version \u001B[91m1.22.4\u001B[00m\n",
      "Package \u001B[91msklearn\u001B[00m, version \u001B[91m1.2.1\u001B[00m\n",
      "Package \u001B[91mtorch\u001B[00m, version \u001B[91m1.12.1+cu113\u001B[00m\n",
      "Package \u001B[91mtorch_geometric\u001B[00m, version \u001B[91m2.3.1\u001B[00m\n",
      "Package \u001B[91mtorch_cluster\u001B[00m, version \u001B[91m1.6.0+pt112cu113\u001B[00m\n",
      "Package \u001B[91mtorch_sparse\u001B[00m, version \u001B[91m0.6.16+pt112cu113\u001B[00m\n",
      "Package \u001B[91mogb\u001B[00m, version \u001B[91m1.3.5\u001B[00m\n"
     ]
    }
   ],
   "source": [
    "import importlib\n",
    "\n",
    "def red(text):\n",
    "    return f'\\033[91m{text}\\033[00m'\n",
    "\n",
    "for package in ['numpy', 'sklearn', 'torch', 'torch_geometric', 'torch_cluster',  'torch_sparse', 'ogb']:\n",
    "    module = importlib.import_module(package)\n",
    "    print(f\"Package {red(package)}, version {red(module.__version__)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91c9a4d1",
   "metadata": {},
   "source": [
    "## Node classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b78eeb16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Pre-training #01] loss = 0.1009, AUC/AP: val = 94.49%/92.90%, test = 95.00%/95.\n",
      "Training ends by early stopping at ep 880.\n",
      "[Test] best AUC: val = 95.15%, test = 95.32%\n",
      "[Test] best AP: val = 93.77%, test = 96.02%\n",
      "[Linear probing #01] loss = 0.5961, Micro-F1/Macro-F1: val = 81.20%/80.00%, test\n",
      "[Test] best Micro-F1: val = 81.40%, test = 84.70%\n",
      "[Test] best Macro-F1: val = 80.14%, test = 83.67%\n",
      "[Linear probing #02] loss = 0.5962, Micro-F1/Macro-F1: val = 81.00%/79.69%, test\n",
      "[Test] best Micro-F1: val = 81.40%, test = 84.70%\n",
      "[Test] best Macro-F1: val = 80.30%, test = 82.90%\n",
      "[Linear probing #03] loss = 0.5961, Micro-F1/Macro-F1: val = 81.20%/80.00%, test\n",
      "[Test] best Micro-F1: val = 81.80%, test = 84.80%\n",
      "[Test] best Macro-F1: val = 80.55%, test = 83.75%\n",
      "[Linear probing #04] loss = 0.5965, Micro-F1/Macro-F1: val = 81.00%/79.69%, test\n",
      "[Test] best Micro-F1: val = 81.60%, test = 83.60%\n",
      "[Test] best Macro-F1: val = 80.72%, test = 82.64%\n",
      "[Linear probing #05] loss = 0.5963, Micro-F1/Macro-F1: val = 81.00%/79.69%, test\n",
      "[Test] best Micro-F1: val = 81.20%, test = 84.70%\n",
      "[Test] best Macro-F1: val = 80.14%, test = 80.84%\n",
      "[Linear probing #06] loss = 0.5963, Micro-F1/Macro-F1: val = 81.00%/79.69%, test\n",
      "[Test] best Micro-F1: val = 81.60%, test = 84.80%\n",
      "[Test] best Macro-F1: val = 80.39%, test = 83.80%\n",
      "[Linear probing #07] loss = 0.5961, Micro-F1/Macro-F1: val = 81.20%/80.00%, test\n",
      "[Test] best Micro-F1: val = 81.60%, test = 84.50%\n",
      "[Test] best Macro-F1: val = 80.35%, test = 83.49%\n",
      "[Linear probing #08] loss = 0.5959, Micro-F1/Macro-F1: val = 81.20%/80.00%, test\n",
      "[Test] best Micro-F1: val = 81.40%, test = 84.80%\n",
      "[Test] best Macro-F1: val = 80.27%, test = 83.68%\n",
      "[Linear probing #09] loss = 0.5964, Micro-F1/Macro-F1: val = 81.20%/80.00%, test\n",
      "[Test] best Micro-F1: val = 81.20%, test = 84.70%\n",
      "[Test] best Macro-F1: val = 80.20%, test = 82.48%\n",
      "[Linear probing #10] loss = 0.5961, Micro-F1/Macro-F1: val = 81.20%/80.00%, test\n",
      "[Test] best Micro-F1: val = 81.40%, test = 84.90%\n",
      "[Test] best Macro-F1: val = 80.23%, test = 82.49%\n",
      "\n",
      "\n",
      " Final MICRO-F1(%): val = 81.46 ± 0.19, test = 84.62 ± 0.37\n",
      " Final MACRO-F1(%): val = 80.33 ± 0.19, test = 82.97 ± 0.92\n"
     ]
    }
   ],
   "source": [
    "!python train_node.py --dataset=Cora --use_cfg --device=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9a70dc65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Pre-training #01] loss = 0.1596, AUC/AP: val = 95.43%/95.71%, test = 94.90%/95.\n",
      "Training ends by early stopping at ep 840.\n",
      "[Test] best AUC: val = 95.76%, test = 96.08%\n",
      "[Test] best AP: val = 96.13%, test = 96.00%\n",
      "[Linear probing #01] loss = 1.5566, Micro-F1/Macro-F1: val = 69.80%/61.81%, test\n",
      "[Test] best Micro-F1: val = 70.40%, test = 73.70%\n",
      "[Test] best Macro-F1: val = 64.04%, test = 67.56%\n",
      "[Linear probing #02] loss = 1.5566, Micro-F1/Macro-F1: val = 69.80%/61.81%, test\n",
      "[Test] best Micro-F1: val = 70.20%, test = 73.80%\n",
      "[Test] best Macro-F1: val = 62.72%, test = 67.72%\n",
      "[Linear probing #03] loss = 1.5566, Micro-F1/Macro-F1: val = 69.80%/61.81%, test\n",
      "[Test] best Micro-F1: val = 70.60%, test = 73.50%\n",
      "[Test] best Macro-F1: val = 63.44%, test = 67.57%\n",
      "[Linear probing #04] loss = 1.5566, Micro-F1/Macro-F1: val = 69.80%/61.81%, test\n",
      "[Test] best Micro-F1: val = 70.20%, test = 73.50%\n",
      "[Test] best Macro-F1: val = 62.88%, test = 68.03%\n",
      "[Linear probing #05] loss = 1.5566, Micro-F1/Macro-F1: val = 69.80%/61.81%, test\n",
      "[Test] best Micro-F1: val = 70.40%, test = 73.70%\n",
      "[Test] best Macro-F1: val = 62.76%, test = 67.82%\n",
      "[Linear probing #06] loss = 1.5566, Micro-F1/Macro-F1: val = 69.80%/61.81%, test\n",
      "[Test] best Micro-F1: val = 70.60%, test = 73.30%\n",
      "[Test] best Macro-F1: val = 63.36%, test = 68.18%\n",
      "[Linear probing #07] loss = 1.5566, Micro-F1/Macro-F1: val = 69.80%/61.81%, test\n",
      "[Test] best Micro-F1: val = 70.40%, test = 73.60%\n",
      "[Test] best Macro-F1: val = 63.47%, test = 68.73%\n",
      "[Linear probing #08] loss = 1.5566, Micro-F1/Macro-F1: val = 69.80%/61.81%, test\n",
      "[Test] best Micro-F1: val = 70.60%, test = 73.80%\n",
      "[Test] best Macro-F1: val = 63.07%, test = 68.22%\n",
      "[Linear probing #09] loss = 1.5566, Micro-F1/Macro-F1: val = 69.80%/61.81%, test\n",
      "[Test] best Micro-F1: val = 70.40%, test = 73.50%\n",
      "[Test] best Macro-F1: val = 63.12%, test = 69.03%\n",
      "[Linear probing #10] loss = 1.5566, Micro-F1/Macro-F1: val = 69.80%/61.81%, test\n",
      "[Test] best Micro-F1: val = 70.20%, test = 73.60%\n",
      "[Test] best Macro-F1: val = 62.92%, test = 68.21%\n",
      "\n",
      "\n",
      " Final MICRO-F1(%): val = 70.40 ± 0.16, test = 73.60 ± 0.16\n",
      " Final MACRO-F1(%): val = 63.18 ± 0.41, test = 68.11 ± 0.48\n"
     ]
    }
   ],
   "source": [
    "!python train_node.py --dataset=Citeseer --use_cfg --device=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b7f88852",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Pre-training #01] loss = 0.5576, AUC/AP: val = 95.80%/96.20%, test = 95.86%/96.\n",
      "Training ends by early stopping at ep 820.\n",
      "[Test] best AUC: val = 95.90%, test = 95.97%\n",
      "[Test] best AP: val = 96.25%, test = 96.62%\n",
      "[Linear probing #01] loss = 0.1667, Micro-F1/Macro-F1: val = 84.40%/84.52%, test\n",
      "[Test] best Micro-F1: val = 85.40%, test = 83.10%\n",
      "[Test] best Macro-F1: val = 85.52%, test = 82.63%\n",
      "[Linear probing #02] loss = 0.1707, Micro-F1/Macro-F1: val = 84.40%/84.46%, test\n",
      "[Test] best Micro-F1: val = 85.80%, test = 84.20%\n",
      "[Test] best Macro-F1: val = 85.75%, test = 83.51%\n",
      "[Linear probing #03] loss = 0.1652, Micro-F1/Macro-F1: val = 84.60%/84.69%, test\n",
      "[Test] best Micro-F1: val = 85.80%, test = 84.20%\n",
      "[Test] best Macro-F1: val = 85.76%, test = 83.40%\n",
      "[Linear probing #04] loss = 0.1691, Micro-F1/Macro-F1: val = 84.60%/84.62%, test\n",
      "[Test] best Micro-F1: val = 85.60%, test = 84.00%\n",
      "[Test] best Macro-F1: val = 85.50%, test = 83.12%\n",
      "[Linear probing #05] loss = 0.1663, Micro-F1/Macro-F1: val = 83.80%/83.95%, test\n",
      "[Test] best Micro-F1: val = 85.40%, test = 83.10%\n",
      "[Test] best Macro-F1: val = 85.52%, test = 82.70%\n",
      "[Linear probing #06] loss = 0.1671, Micro-F1/Macro-F1: val = 84.20%/84.29%, test\n",
      "[Test] best Micro-F1: val = 85.20%, test = 83.30%\n",
      "[Test] best Macro-F1: val = 85.35%, test = 82.80%\n",
      "[Linear probing #07] loss = 0.1630, Micro-F1/Macro-F1: val = 83.80%/84.02%, test\n",
      "[Test] best Micro-F1: val = 84.60%, test = 83.70%\n",
      "[Test] best Macro-F1: val = 84.92%, test = 83.46%\n",
      "[Linear probing #08] loss = 0.1681, Micro-F1/Macro-F1: val = 84.20%/84.42%, test\n",
      "[Test] best Micro-F1: val = 84.60%, test = 83.80%\n",
      "[Test] best Macro-F1: val = 84.83%, test = 83.24%\n",
      "[Linear probing #09] loss = 0.1693, Micro-F1/Macro-F1: val = 83.80%/83.89%, test\n",
      "[Test] best Micro-F1: val = 84.80%, test = 83.00%\n",
      "[Test] best Macro-F1: val = 85.09%, test = 82.46%\n",
      "[Linear probing #10] loss = 0.1710, Micro-F1/Macro-F1: val = 84.40%/84.46%, test\n",
      "[Test] best Micro-F1: val = 84.80%, test = 82.90%\n",
      "[Test] best Macro-F1: val = 84.95%, test = 82.55%\n",
      "\n",
      "\n",
      " Final MICRO-F1(%): val = 85.20 ± 0.47, test = 83.53 ± 0.51\n",
      " Final MACRO-F1(%): val = 85.32 ± 0.35, test = 82.99 ± 0.40\n"
     ]
    }
   ],
   "source": [
    "!python train_node.py --dataset=Pubmed --use_cfg --device=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "89a0eafe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Pre-training #01] loss = 0.3180, AUC/AP: val = 96.04%/95.94%, test = 96.02%/95.\n",
      "Training ends by early stopping at ep 790.\n",
      "[Test] best AUC: val = 96.42%, test = 96.45%\n",
      "[Test] best AP: val = 96.34%, test = 96.16%\n",
      "[Linear probing #01] loss = 0.2617, Micro-F1/Macro-F1: val = 94.51%/93.09%, test\n",
      "[Test] best Micro-F1: val = 94.64%, test = 93.51%\n",
      "[Test] best Macro-F1: val = 93.38%, test = 92.28%\n",
      "[Linear probing #02] loss = 0.2533, Micro-F1/Macro-F1: val = 94.38%/92.87%, test\n",
      "[Test] best Micro-F1: val = 94.64%, test = 93.55%\n",
      "[Test] best Macro-F1: val = 93.54%, test = 92.26%\n",
      "[Linear probing #03] loss = 0.2560, Micro-F1/Macro-F1: val = 94.77%/93.37%, test\n",
      "[Test] best Micro-F1: val = 95.03%, test = 93.50%\n",
      "[Test] best Macro-F1: val = 93.61%, test = 92.32%\n",
      "[Linear probing #04] loss = 0.2317, Micro-F1/Macro-F1: val = 94.38%/92.86%, test\n",
      "[Test] best Micro-F1: val = 94.90%, test = 93.48%\n",
      "[Test] best Macro-F1: val = 93.86%, test = 92.30%\n",
      "[Linear probing #05] loss = 0.2417, Micro-F1/Macro-F1: val = 94.38%/92.67%, test\n",
      "[Test] best Micro-F1: val = 94.77%, test = 93.53%\n",
      "[Test] best Macro-F1: val = 93.40%, test = 92.28%\n",
      "[Linear probing #06] loss = 0.2492, Micro-F1/Macro-F1: val = 94.12%/92.50%, test\n",
      "[Test] best Micro-F1: val = 94.64%, test = 93.19%\n",
      "[Test] best Macro-F1: val = 93.23%, test = 92.26%\n",
      "[Linear probing #07] loss = 0.2572, Micro-F1/Macro-F1: val = 94.77%/93.40%, test\n",
      "[Test] best Micro-F1: val = 94.77%, test = 93.37%\n",
      "[Test] best Macro-F1: val = 93.40%, test = 92.27%\n",
      "[Linear probing #08] loss = 0.2584, Micro-F1/Macro-F1: val = 94.64%/93.23%, test\n",
      "[Test] best Micro-F1: val = 95.03%, test = 93.51%\n",
      "[Test] best Macro-F1: val = 93.63%, test = 92.23%\n",
      "[Linear probing #09] loss = 0.2488, Micro-F1/Macro-F1: val = 94.38%/93.31%, test\n",
      "[Test] best Micro-F1: val = 94.51%, test = 93.43%\n",
      "[Test] best Macro-F1: val = 93.40%, test = 92.24%\n",
      "[Linear probing #10] loss = 0.2515, Micro-F1/Macro-F1: val = 94.38%/92.73%, test\n",
      "[Test] best Micro-F1: val = 94.77%, test = 93.38%\n",
      "[Test] best Macro-F1: val = 93.51%, test = 92.19%\n",
      "\n",
      "\n",
      " Final MICRO-F1(%): val = 94.77 ± 0.17, test = 93.44 ± 0.11\n",
      " Final MACRO-F1(%): val = 93.50 ± 0.17, test = 92.26 ± 0.04\n"
     ]
    }
   ],
   "source": [
    "!python train_node.py --dataset=Photo --use_cfg --device=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "849aa36e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Pre-training #01] loss = 0.4578, AUC/AP: val = 93.40%/94.56%, test = 93.95%/95.\n",
      "Training ends by early stopping at ep 620.\n",
      "[Test] best AUC: val = 95.18%, test = 95.58%\n",
      "[Test] best AP: val = 95.80%, test = 96.26%\n",
      "[Linear probing #01] loss = 0.2321, Micro-F1/Macro-F1: val = 87.93%/85.08%, test\n",
      "[Test] best Micro-F1: val = 88.29%, test = 89.43%\n",
      "[Test] best Macro-F1: val = 86.24%, test = 88.00%\n",
      "[Linear probing #02] loss = 0.2385, Micro-F1/Macro-F1: val = 88.15%/85.24%, test\n",
      "[Test] best Micro-F1: val = 88.29%, test = 89.58%\n",
      "[Test] best Macro-F1: val = 85.86%, test = 88.00%\n",
      "[Linear probing #03] loss = 0.2386, Micro-F1/Macro-F1: val = 88.15%/85.09%, test\n",
      "[Test] best Micro-F1: val = 88.22%, test = 89.69%\n",
      "[Test] best Macro-F1: val = 86.04%, test = 87.51%\n",
      "[Linear probing #04] loss = 0.2373, Micro-F1/Macro-F1: val = 88.07%/85.71%, test\n",
      "[Test] best Micro-F1: val = 88.29%, test = 89.64%\n",
      "[Test] best Macro-F1: val = 85.92%, test = 88.09%\n",
      "[Linear probing #05] loss = 0.2417, Micro-F1/Macro-F1: val = 87.85%/85.35%, test\n",
      "[Test] best Micro-F1: val = 88.29%, test = 89.74%\n",
      "[Test] best Macro-F1: val = 86.06%, test = 87.80%\n",
      "[Linear probing #06] loss = 0.2424, Micro-F1/Macro-F1: val = 87.64%/84.77%, test\n",
      "[Test] best Micro-F1: val = 88.22%, test = 89.57%\n",
      "[Test] best Macro-F1: val = 86.26%, test = 87.60%\n",
      "[Linear probing #07] loss = 0.2368, Micro-F1/Macro-F1: val = 87.85%/84.24%, test\n",
      "[Test] best Micro-F1: val = 88.44%, test = 89.67%\n",
      "[Test] best Macro-F1: val = 85.89%, test = 87.79%\n",
      "[Linear probing #08] loss = 0.2445, Micro-F1/Macro-F1: val = 88.07%/85.36%, test\n",
      "[Test] best Micro-F1: val = 88.22%, test = 89.63%\n",
      "[Test] best Macro-F1: val = 86.00%, test = 87.72%\n",
      "[Linear probing #09] loss = 0.2369, Micro-F1/Macro-F1: val = 87.56%/85.18%, test\n",
      "[Test] best Micro-F1: val = 88.29%, test = 89.52%\n",
      "[Test] best Macro-F1: val = 85.97%, test = 87.55%\n",
      "[Linear probing #10] loss = 0.2422, Micro-F1/Macro-F1: val = 87.56%/84.84%, test\n",
      "[Test] best Micro-F1: val = 88.29%, test = 89.68%\n",
      "[Test] best Macro-F1: val = 86.12%, test = 87.86%\n",
      "\n",
      "\n",
      " Final MICRO-F1(%): val = 88.28 ± 0.06, test = 89.62 ± 0.09\n",
      " Final MACRO-F1(%): val = 86.04 ± 0.14, test = 87.79 ± 0.20\n"
     ]
    }
   ],
   "source": [
    "!python train_node.py --dataset=Computers --use_cfg --device=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eb1b8e6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Pre-training #01] loss = 0.6414, AUC/AP: val = 95.47%/95.03%, test = 95.73%/95.\n",
      "Training ends by early stopping at ep 530.\n",
      "[Test] best AUC: val = 95.58%, test = 95.83%\n",
      "[Test] best AP: val = 94.98%, test = 95.21%\n",
      "[Linear probing #01] loss = 0.7135, Micro-F1/Macro-F1: val = 93.51%/91.66%, test\n",
      "[Test] best Micro-F1: val = 93.56%, test = 93.13%\n",
      "[Test] best Macro-F1: val = 91.87%, test = 91.15%\n",
      "[Linear probing #02] loss = 0.7182, Micro-F1/Macro-F1: val = 93.18%/91.13%, test\n",
      "[Test] best Micro-F1: val = 93.62%, test = 93.09%\n",
      "[Test] best Macro-F1: val = 91.79%, test = 91.17%\n",
      "[Linear probing #03] loss = 0.7218, Micro-F1/Macro-F1: val = 93.29%/91.39%, test\n",
      "[Test] best Micro-F1: val = 93.51%, test = 93.13%\n",
      "[Test] best Macro-F1: val = 91.72%, test = 90.89%\n",
      "[Linear probing #04] loss = 0.7066, Micro-F1/Macro-F1: val = 93.18%/91.32%, test\n",
      "[Test] best Micro-F1: val = 93.45%, test = 93.13%\n",
      "[Test] best Macro-F1: val = 91.70%, test = 90.95%\n",
      "[Linear probing #05] loss = 0.7050, Micro-F1/Macro-F1: val = 93.29%/91.22%, test\n",
      "[Test] best Micro-F1: val = 93.62%, test = 93.10%\n",
      "[Test] best Macro-F1: val = 91.88%, test = 91.07%\n",
      "[Linear probing #06] loss = 0.7217, Micro-F1/Macro-F1: val = 93.45%/91.45%, test\n",
      "[Test] best Micro-F1: val = 93.62%, test = 93.04%\n",
      "[Test] best Macro-F1: val = 91.75%, test = 91.04%\n",
      "[Linear probing #07] loss = 0.7229, Micro-F1/Macro-F1: val = 93.18%/91.12%, test\n",
      "[Test] best Micro-F1: val = 93.62%, test = 93.12%\n",
      "[Test] best Macro-F1: val = 91.69%, test = 91.03%\n",
      "[Linear probing #08] loss = 0.7011, Micro-F1/Macro-F1: val = 93.07%/90.95%, test\n",
      "[Test] best Micro-F1: val = 93.62%, test = 93.03%\n",
      "[Test] best Macro-F1: val = 91.76%, test = 90.92%\n",
      "[Linear probing #09] loss = 0.7225, Micro-F1/Macro-F1: val = 93.18%/91.13%, test\n",
      "[Test] best Micro-F1: val = 93.56%, test = 93.07%\n",
      "[Test] best Macro-F1: val = 91.73%, test = 90.83%\n",
      "[Linear probing #10] loss = 0.7013, Micro-F1/Macro-F1: val = 93.18%/91.22%, test\n",
      "[Test] best Micro-F1: val = 93.45%, test = 93.18%\n",
      "[Test] best Macro-F1: val = 91.71%, test = 91.19%\n",
      "\n",
      "\n",
      " Final MICRO-F1(%): val = 93.56 ± 0.07, test = 93.10 ± 0.05\n",
      " Final MACRO-F1(%): val = 91.76 ± 0.07, test = 91.02 ± 0.13\n"
     ]
    }
   ],
   "source": [
    "!python train_node.py --dataset=CS --use_cfg --device=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bbba144e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Pre-training #01] loss = 0.7423, AUC/AP: val = 97.54%/97.40%, test = 97.47%/97.\n",
      "[Test] best AUC: val = 97.58%, test = 97.51%\n",
      "[Test] best AP: val = 97.44%, test = 97.37%\n",
      "[Linear probing #01] loss = 0.5803, Micro-F1/Macro-F1: val = 95.94%/94.66%, test\n",
      "[Test] best Micro-F1: val = 96.17%, test = 95.53%\n",
      "[Test] best Macro-F1: val = 94.95%, test = 94.22%\n",
      "[Linear probing #02] loss = 0.5906, Micro-F1/Macro-F1: val = 96.00%/94.65%, test\n",
      "[Test] best Micro-F1: val = 96.14%, test = 95.58%\n",
      "[Test] best Macro-F1: val = 94.93%, test = 94.21%\n",
      "[Linear probing #03] loss = 0.5901, Micro-F1/Macro-F1: val = 95.88%/94.51%, test\n",
      "[Test] best Micro-F1: val = 96.17%, test = 95.58%\n",
      "[Test] best Macro-F1: val = 94.94%, test = 94.18%\n",
      "[Linear probing #04] loss = 0.5853, Micro-F1/Macro-F1: val = 95.71%/94.35%, test\n",
      "[Test] best Micro-F1: val = 96.14%, test = 95.54%\n",
      "[Test] best Macro-F1: val = 94.90%, test = 94.18%\n",
      "[Linear probing #05] loss = 0.5845, Micro-F1/Macro-F1: val = 96.00%/94.73%, test\n",
      "[Test] best Micro-F1: val = 96.09%, test = 95.63%\n",
      "[Test] best Macro-F1: val = 94.89%, test = 94.18%\n",
      "[Linear probing #06] loss = 0.5911, Micro-F1/Macro-F1: val = 95.91%/94.65%, test\n",
      "[Test] best Micro-F1: val = 96.14%, test = 95.50%\n",
      "[Test] best Macro-F1: val = 94.92%, test = 94.11%\n",
      "[Linear probing #07] loss = 0.5799, Micro-F1/Macro-F1: val = 95.94%/94.72%, test\n",
      "[Test] best Micro-F1: val = 96.11%, test = 95.63%\n",
      "[Test] best Macro-F1: val = 94.89%, test = 94.31%\n",
      "[Linear probing #08] loss = 0.5760, Micro-F1/Macro-F1: val = 95.97%/94.70%, test\n",
      "[Test] best Micro-F1: val = 96.14%, test = 95.58%\n",
      "[Test] best Macro-F1: val = 94.89%, test = 94.26%\n",
      "[Linear probing #09] loss = 0.5727, Micro-F1/Macro-F1: val = 95.94%/94.67%, test\n",
      "[Test] best Micro-F1: val = 96.20%, test = 95.54%\n",
      "[Test] best Macro-F1: val = 95.00%, test = 94.18%\n",
      "[Linear probing #10] loss = 0.5790, Micro-F1/Macro-F1: val = 95.85%/94.56%, test\n",
      "[Test] best Micro-F1: val = 96.14%, test = 95.60%\n",
      "[Test] best Macro-F1: val = 94.96%, test = 94.20%\n",
      "\n",
      "\n",
      " Final MICRO-F1(%): val = 96.15 ± 0.03, test = 95.57 ± 0.04\n",
      " Final MACRO-F1(%): val = 94.93 ± 0.04, test = 94.20 ± 0.05\n"
     ]
    }
   ],
   "source": [
    "!python train_node.py --dataset=Physics --use_cfg --device=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9fabc144",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:root:The OGB package is out of date. Your version is 1.3.5, while the latest version is 1.3.6.\n",
      "Loading ogb dataset...\n",
      "[Pre-training #01] loss = 2.2224, AUC/AP: val = 90.40%/93.49%, test = 90.32%/93.\n",
      "[Test] best AUC: val = 95.32%, test = 95.42%\n",
      "[Test] best AP: val = 96.60%, test = 96.68%\n",
      "[Linear probing #01] loss = 18.8961, Micro-F1/Macro-F1: val = 71.14%/50.33%, tes\n",
      "[Test] best Micro-F1: val = 71.64%, test = 71.30%\n",
      "[Test] best Macro-F1: val = 51.46%, test = 48.80%\n",
      "[Linear probing #02] loss = 18.8330, Micro-F1/Macro-F1: val = 70.54%/50.68%, tes\n",
      "[Test] best Micro-F1: val = 71.51%, test = 70.82%\n",
      "[Test] best Macro-F1: val = 51.59%, test = 49.86%\n",
      "[Linear probing #03] loss = 18.8287, Micro-F1/Macro-F1: val = 71.01%/50.24%, tes\n",
      "[Test] best Micro-F1: val = 71.63%, test = 70.70%\n",
      "[Test] best Macro-F1: val = 51.87%, test = 50.41%\n",
      "[Linear probing #04] loss = 18.8174, Micro-F1/Macro-F1: val = 70.47%/50.02%, tes\n",
      "[Test] best Micro-F1: val = 71.63%, test = 71.42%\n",
      "[Test] best Macro-F1: val = 51.06%, test = 49.93%\n",
      "[Linear probing #05] loss = 18.7877, Micro-F1/Macro-F1: val = 70.93%/50.35%, tes\n",
      "[Test] best Micro-F1: val = 71.60%, test = 71.17%\n",
      "[Test] best Macro-F1: val = 51.14%, test = 49.00%\n",
      "[Linear probing #06] loss = 18.7760, Micro-F1/Macro-F1: val = 70.83%/50.96%, tes\n",
      "[Test] best Micro-F1: val = 71.96%, test = 71.18%\n",
      "[Test] best Macro-F1: val = 51.52%, test = 49.85%\n",
      "[Linear probing #07] loss = 18.8751, Micro-F1/Macro-F1: val = 71.37%/49.88%, tes\n",
      "[Test] best Micro-F1: val = 71.69%, test = 70.98%\n",
      "[Test] best Macro-F1: val = 51.34%, test = 49.29%\n",
      "[Linear probing #08] loss = 18.8099, Micro-F1/Macro-F1: val = 70.76%/49.76%, tes\n",
      "[Test] best Micro-F1: val = 71.61%, test = 71.01%\n",
      "[Test] best Macro-F1: val = 51.18%, test = 49.67%\n",
      "[Linear probing #09] loss = 18.8343, Micro-F1/Macro-F1: val = 71.11%/50.31%, tes\n",
      "[Test] best Micro-F1: val = 71.65%, test = 71.40%\n",
      "[Test] best Macro-F1: val = 51.10%, test = 49.64%\n",
      "[Linear probing #10] loss = 18.7738, Micro-F1/Macro-F1: val = 71.35%/50.21%, tes\n",
      "[Test] best Micro-F1: val = 71.45%, test = 70.94%\n",
      "[Test] best Macro-F1: val = 51.68%, test = 50.14%\n",
      "\n",
      "\n",
      " Final MICRO-F1(%): val = 71.64 ± 0.13, test = 71.09 ± 0.24\n",
      " Final MACRO-F1(%): val = 51.39 ± 0.27, test = 49.66 ± 0.50\n"
     ]
    }
   ],
   "source": [
    "!python train_node.py --dataset=ogbn-arxiv --use_cfg --device=0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33e189e6",
   "metadata": {},
   "source": [
    "## Link prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "30221454",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Pre-training #01] loss = 0.1339, AUC/AP: val = 95.94%/95.02%, test = 95.81%/95.\n",
      "Training ends by early stopping at ep 910.\n",
      "[Test] best AUC: val = 96.39%, test = 95.74%\n",
      "[Test] best AP: val = 95.47%, test = 95.33%\n",
      "[Pre-training #02] loss = 0.1351, AUC/AP: val = 95.80%/94.68%, test = 95.42%/94.\n",
      "[Test] best AUC: val = 96.55%, test = 95.74%\n",
      "[Test] best AP: val = 95.37%, test = 95.40%\n",
      "[Pre-training #03] loss = 0.1395, AUC/AP: val = 95.34%/94.50%, test = 95.77%/95.\n",
      "[Test] best AUC: val = 96.20%, test = 95.87%\n",
      "[Test] best AP: val = 95.10%, test = 95.45%\n",
      "[Pre-training #04] loss = 0.1466, AUC/AP: val = 96.11%/95.24%, test = 95.67%/95.\n",
      "Training ends by early stopping at ep 870.\n",
      "[Test] best AUC: val = 96.36%, test = 95.85%\n",
      "[Test] best AP: val = 95.49%, test = 95.31%\n",
      "[Pre-training #05] loss = 0.1395, AUC/AP: val = 95.55%/94.71%, test = 95.92%/95.\n",
      "Training ends by early stopping at ep 850.\n",
      "[Test] best AUC: val = 96.40%, test = 95.60%\n",
      "[Test] best AP: val = 95.48%, test = 95.23%\n",
      "[Pre-training #06] loss = 0.1392, AUC/AP: val = 95.76%/94.82%, test = 95.49%/94.\n",
      "Training ends by early stopping at ep 970.\n",
      "[Test] best AUC: val = 96.26%, test = 95.58%\n",
      "[Test] best AP: val = 95.41%, test = 95.17%\n",
      "[Pre-training #07] loss = 0.1480, AUC/AP: val = 95.69%/95.03%, test = 95.82%/95.\n",
      "Training ends by early stopping at ep 740.\n",
      "[Test] best AUC: val = 96.25%, test = 95.80%\n",
      "[Test] best AP: val = 95.38%, test = 95.38%\n",
      "[Pre-training #08] loss = 0.1394, AUC/AP: val = 96.22%/95.29%, test = 95.44%/95.\n",
      "Training ends by early stopping at ep 840.\n",
      "[Test] best AUC: val = 96.50%, test = 95.75%\n",
      "[Test] best AP: val = 95.50%, test = 95.02%\n",
      "[Pre-training #09] loss = 0.1425, AUC/AP: val = 95.86%/94.83%, test = 95.74%/95.\n",
      "Training ends by early stopping at ep 740.\n",
      "[Test] best AUC: val = 96.68%, test = 95.65%\n",
      "[Test] best AP: val = 95.88%, test = 95.00%\n",
      "[Pre-training #10] loss = 0.1402, AUC/AP: val = 96.13%/95.18%, test = 95.61%/95.\n",
      "[Test] best AUC: val = 96.38%, test = 95.52%\n",
      "[Test] best AP: val = 95.24%, test = 95.18%\n",
      "\n",
      "\n",
      " Final AUC(%): val = 96.40 ± 0.15, test = 95.71 ± 0.12\n",
      " Final AP(%): val = 95.43 ± 0.20, test = 95.25 ± 0.16\n"
     ]
    }
   ],
   "source": [
    "!python train_link.py --dataset=Cora --use_cfg --device=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d6e21ba8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Pre-training #01] loss = 0.1068, AUC/AP: val = 96.44%/96.92%, test = 96.52%/97.\n",
      "Training ends by early stopping at ep 830.\n",
      "[Test] best AUC: val = 96.85%, test = 96.52%\n",
      "[Test] best AP: val = 97.24%, test = 97.03%\n",
      "[Pre-training #02] loss = 0.1213, AUC/AP: val = 96.91%/97.28%, test = 96.99%/97.\n",
      "Training ends by early stopping at ep 770.\n",
      "[Test] best AUC: val = 97.29%, test = 96.88%\n",
      "[Test] best AP: val = 97.55%, test = 97.05%\n",
      "[Pre-training #03] loss = 0.1196, AUC/AP: val = 96.71%/97.24%, test = 96.68%/97.\n",
      "Training ends by early stopping at ep 970.\n",
      "[Test] best AUC: val = 96.95%, test = 96.92%\n",
      "[Test] best AP: val = 97.39%, test = 97.28%\n",
      "[Pre-training #04] loss = 0.1048, AUC/AP: val = 96.62%/97.08%, test = 96.98%/97.\n",
      "Training ends by early stopping at ep 770.\n",
      "[Test] best AUC: val = 96.87%, test = 96.97%\n",
      "[Test] best AP: val = 97.26%, test = 97.33%\n",
      "[Pre-training #05] loss = 0.1123, AUC/AP: val = 96.47%/96.98%, test = 96.94%/97.\n",
      "Training ends by early stopping at ep 500.\n",
      "[Test] best AUC: val = 96.85%, test = 97.10%\n",
      "[Test] best AP: val = 97.25%, test = 97.15%\n",
      "[Pre-training #06] loss = 0.1121, AUC/AP: val = 96.26%/96.83%, test = 97.03%/97.\n",
      "Training ends by early stopping at ep 920.\n",
      "[Test] best AUC: val = 96.80%, test = 96.96%\n",
      "[Test] best AP: val = 97.16%, test = 97.04%\n",
      "[Pre-training #07] loss = 0.1142, AUC/AP: val = 96.50%/96.99%, test = 97.05%/97.\n",
      "Training ends by early stopping at ep 770.\n",
      "[Test] best AUC: val = 96.91%, test = 97.20%\n",
      "[Test] best AP: val = 97.29%, test = 97.47%\n",
      "[Pre-training #08] loss = 0.1283, AUC/AP: val = 96.50%/97.08%, test = 96.86%/97.\n",
      "Training ends by early stopping at ep 500.\n",
      "[Test] best AUC: val = 96.86%, test = 96.97%\n",
      "[Test] best AP: val = 97.28%, test = 97.15%\n",
      "[Pre-training #09] loss = 0.1027, AUC/AP: val = 96.45%/97.05%, test = 96.67%/97.\n",
      "Training ends by early stopping at ep 800.\n",
      "[Test] best AUC: val = 96.89%, test = 96.78%\n",
      "[Test] best AP: val = 97.27%, test = 97.21%\n",
      "[Pre-training #10] loss = 0.1111, AUC/AP: val = 96.75%/97.16%, test = 96.87%/97.\n",
      "Training ends by early stopping at ep 680.\n",
      "[Test] best AUC: val = 96.84%, test = 96.61%\n",
      "[Test] best AP: val = 97.31%, test = 96.88%\n",
      "\n",
      "\n",
      " Final AUC(%): val = 96.91 ± 0.14, test = 96.89 ± 0.21\n",
      " Final AP(%): val = 97.30 ± 0.10, test = 97.16 ± 0.17\n"
     ]
    }
   ],
   "source": [
    "!python train_link.py --dataset=Citeseer --use_cfg --device=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b4ec4cbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Pre-training #01] loss = 0.3043, AUC/AP: val = 96.52%/96.30%, test = 96.98%/96.\n",
      "Training ends by early stopping at ep 390.\n",
      "[Test] best AUC: val = 97.02%, test = 97.41%\n",
      "[Test] best AP: val = 96.51%, test = 97.00%\n",
      "[Pre-training #02] loss = 0.3103, AUC/AP: val = 96.31%/96.16%, test = 96.71%/96.\n",
      "Training ends by early stopping at ep 370.\n",
      "[Test] best AUC: val = 96.91%, test = 97.26%\n",
      "[Test] best AP: val = 96.21%, test = 96.61%\n",
      "[Pre-training #03] loss = 0.3083, AUC/AP: val = 96.32%/96.15%, test = 96.60%/96.\n",
      "Training ends by early stopping at ep 370.\n",
      "[Test] best AUC: val = 97.15%, test = 97.55%\n",
      "[Test] best AP: val = 96.63%, test = 97.07%\n",
      "[Pre-training #04] loss = 0.3061, AUC/AP: val = 96.29%/96.13%, test = 96.65%/96.\n",
      "Training ends by early stopping at ep 410.\n",
      "[Test] best AUC: val = 96.93%, test = 97.25%\n",
      "[Test] best AP: val = 96.49%, test = 96.90%\n",
      "[Pre-training #05] loss = 0.3104, AUC/AP: val = 96.44%/96.31%, test = 96.89%/96.\n",
      "Training ends by early stopping at ep 400.\n",
      "[Test] best AUC: val = 96.98%, test = 97.19%\n",
      "[Test] best AP: val = 96.54%, test = 96.78%\n",
      "[Pre-training #06] loss = 0.3065, AUC/AP: val = 96.58%/96.35%, test = 96.91%/96.\n",
      "Training ends by early stopping at ep 390.\n",
      "[Test] best AUC: val = 96.93%, test = 97.21%\n",
      "[Test] best AP: val = 96.48%, test = 96.87%\n",
      "[Pre-training #07] loss = 0.3108, AUC/AP: val = 96.13%/95.98%, test = 96.49%/96.\n",
      "Training ends by early stopping at ep 380.\n",
      "[Test] best AUC: val = 96.99%, test = 97.18%\n",
      "[Test] best AP: val = 96.41%, test = 96.80%\n",
      "[Pre-training #08] loss = 0.3098, AUC/AP: val = 96.23%/96.04%, test = 96.58%/96.\n",
      "Training ends by early stopping at ep 340.\n",
      "[Test] best AUC: val = 96.79%, test = 96.94%\n",
      "[Test] best AP: val = 95.78%, test = 95.76%\n",
      "[Pre-training #09] loss = 0.3076, AUC/AP: val = 96.19%/95.95%, test = 96.61%/96.\n",
      "Training ends by early stopping at ep 350.\n",
      "[Test] best AUC: val = 97.17%, test = 97.31%\n",
      "[Test] best AP: val = 96.39%, test = 96.65%\n",
      "[Pre-training #10] loss = 0.3065, AUC/AP: val = 96.30%/96.09%, test = 96.70%/96.\n",
      "Training ends by early stopping at ep 400.\n",
      "[Test] best AUC: val = 96.87%, test = 97.31%\n",
      "[Test] best AP: val = 96.45%, test = 96.99%\n",
      "\n",
      "\n",
      " Final AUC(%): val = 96.97 ± 0.12, test = 97.26 ± 0.16\n",
      " Final AP(%): val = 96.39 ± 0.24, test = 96.74 ± 0.38\n"
     ]
    }
   ],
   "source": [
    "!python train_link.py --dataset=Pubmed --use_cfg --device=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "62dd4bf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Pre-training #01] loss = 0.3755, AUC/AP: val = 96.61%/96.22%, test = 96.63%/96.\n",
      "[Test] best AUC: val = 97.37%, test = 97.36%\n",
      "[Test] best AP: val = 97.08%, test = 97.01%\n",
      "[Pre-training #02] loss = 0.3765, AUC/AP: val = 96.28%/95.46%, test = 96.25%/95.\n",
      "Training ends by early stopping at ep 600.\n",
      "[Test] best AUC: val = 97.27%, test = 97.25%\n",
      "[Test] best AP: val = 96.91%, test = 96.81%\n",
      "[Pre-training #03] loss = 0.3784, AUC/AP: val = 96.88%/96.48%, test = 96.92%/96.\n",
      "Training ends by early stopping at ep 540.\n",
      "[Test] best AUC: val = 97.31%, test = 97.25%\n",
      "[Test] best AP: val = 96.99%, test = 96.80%\n",
      "[Pre-training #04] loss = 0.3792, AUC/AP: val = 96.77%/96.25%, test = 96.74%/96.\n",
      "Training ends by early stopping at ep 930.\n",
      "[Test] best AUC: val = 97.43%, test = 97.45%\n",
      "[Test] best AP: val = 97.10%, test = 97.04%\n",
      "[Pre-training #05] loss = 0.3751, AUC/AP: val = 96.26%/95.51%, test = 96.23%/95.\n",
      "Training ends by early stopping at ep 620.\n",
      "[Test] best AUC: val = 97.19%, test = 97.19%\n",
      "[Test] best AP: val = 96.85%, test = 96.63%\n",
      "[Pre-training #06] loss = 0.3772, AUC/AP: val = 97.10%/96.61%, test = 97.06%/96.\n",
      "Training ends by early stopping at ep 520.\n",
      "[Test] best AUC: val = 97.25%, test = 97.19%\n",
      "[Test] best AP: val = 96.92%, test = 96.73%\n",
      "[Pre-training #07] loss = 0.3749, AUC/AP: val = 96.11%/95.19%, test = 96.20%/95.\n",
      "Training ends by early stopping at ep 820.\n",
      "[Test] best AUC: val = 97.21%, test = 97.22%\n",
      "[Test] best AP: val = 96.91%, test = 96.80%\n",
      "[Pre-training #08] loss = 0.3722, AUC/AP: val = 96.95%/96.55%, test = 96.95%/96.\n",
      "Training ends by early stopping at ep 800.\n",
      "[Test] best AUC: val = 97.09%, test = 97.08%\n",
      "[Test] best AP: val = 96.73%, test = 96.61%\n",
      "[Pre-training #09] loss = 0.3758, AUC/AP: val = 97.10%/96.74%, test = 97.06%/96.\n",
      "Training ends by early stopping at ep 960.\n",
      "[Test] best AUC: val = 97.22%, test = 97.25%\n",
      "[Test] best AP: val = 96.86%, test = 96.83%\n",
      "[Pre-training #10] loss = 0.3818, AUC/AP: val = 97.15%/96.85%, test = 97.06%/96.\n",
      "Training ends by early stopping at ep 440.\n",
      "[Test] best AUC: val = 97.19%, test = 97.12%\n",
      "[Test] best AP: val = 96.82%, test = 96.62%\n",
      "\n",
      "\n",
      " Final AUC(%): val = 97.25 ± 0.10, test = 97.24 ± 0.11\n",
      " Final AP(%): val = 96.92 ± 0.11, test = 96.79 ± 0.15\n"
     ]
    }
   ],
   "source": [
    "!python train_link.py --dataset=Photo --use_cfg --device=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "95b3373a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Pre-training #01] loss = 0.4697, AUC/AP: val = 96.46%/96.06%, test = 96.63%/96.\n",
      "Training ends by early stopping at ep 790.\n",
      "[Test] best AUC: val = 97.23%, test = 97.37%\n",
      "[Test] best AP: val = 96.79%, test = 96.92%\n",
      "[Pre-training #02] loss = 0.4725, AUC/AP: val = 96.64%/96.29%, test = 96.87%/96.\n",
      "[Test] best AUC: val = 97.20%, test = 97.43%\n",
      "[Test] best AP: val = 96.85%, test = 97.03%\n",
      "[Pre-training #03] loss = 0.4676, AUC/AP: val = 96.97%/96.57%, test = 97.13%/96.\n",
      "[Test] best AUC: val = 97.21%, test = 97.32%\n",
      "[Test] best AP: val = 96.81%, test = 96.94%\n",
      "[Pre-training #04] loss = 0.4767, AUC/AP: val = 96.35%/95.98%, test = 96.53%/96.\n",
      "Training ends by early stopping at ep 730.\n",
      "[Test] best AUC: val = 97.10%, test = 97.26%\n",
      "[Test] best AP: val = 96.70%, test = 96.82%\n",
      "[Pre-training #05] loss = 0.4711, AUC/AP: val = 96.57%/96.16%, test = 96.74%/96.\n",
      "Training ends by early stopping at ep 990.\n",
      "[Test] best AUC: val = 97.21%, test = 97.38%\n",
      "[Test] best AP: val = 96.82%, test = 97.02%\n",
      "[Pre-training #06] loss = 0.4667, AUC/AP: val = 96.84%/96.35%, test = 96.96%/96.\n",
      "[Test] best AUC: val = 97.15%, test = 97.32%\n",
      "[Test] best AP: val = 96.80%, test = 96.97%\n",
      "[Pre-training #07] loss = 0.4739, AUC/AP: val = 96.81%/96.40%, test = 97.03%/96.\n",
      "Training ends by early stopping at ep 770.\n",
      "[Test] best AUC: val = 97.24%, test = 97.39%\n",
      "[Test] best AP: val = 96.81%, test = 96.94%\n",
      "[Pre-training #08] loss = 0.4751, AUC/AP: val = 96.69%/96.32%, test = 96.91%/96.\n",
      "Training ends by early stopping at ep 730.\n",
      "[Test] best AUC: val = 97.15%, test = 97.26%\n",
      "[Test] best AP: val = 96.74%, test = 96.83%\n",
      "[Pre-training #09] loss = 0.4751, AUC/AP: val = 96.77%/96.54%, test = 96.98%/96.\n",
      "Training ends by early stopping at ep 670.\n",
      "[Test] best AUC: val = 97.17%, test = 97.28%\n",
      "[Test] best AP: val = 96.71%, test = 96.74%\n",
      "[Pre-training #10] loss = 0.4671, AUC/AP: val = 96.53%/96.04%, test = 96.74%/96.\n",
      "[Test] best AUC: val = 97.19%, test = 97.30%\n",
      "[Test] best AP: val = 96.79%, test = 96.91%\n",
      "\n",
      "\n",
      " Final AUC(%): val = 97.18 ± 0.04, test = 97.33 ± 0.06\n",
      " Final AP(%): val = 96.78 ± 0.05, test = 96.91 ± 0.09\n"
     ]
    }
   ],
   "source": [
    "!python train_link.py --dataset=Computers --use_cfg --device=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "942596d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Pre-training #01] loss = 0.2394, AUC/AP: val = 97.06%/96.80%, test = 97.41%/97.\n",
      "[Test] best AUC: val = 97.17%, test = 97.45%\n",
      "[Test] best AP: val = 96.93%, test = 97.23%\n",
      "[Pre-training #02] loss = 0.2386, AUC/AP: val = 97.12%/96.86%, test = 97.32%/96.\n",
      "[Test] best AUC: val = 97.16%, test = 97.35%\n",
      "[Test] best AP: val = 96.87%, test = 96.90%\n",
      "[Pre-training #03] loss = 0.2376, AUC/AP: val = 97.28%/97.09%, test = 97.46%/97.\n",
      "[Test] best AUC: val = 97.29%, test = 97.49%\n",
      "[Test] best AP: val = 97.02%, test = 97.20%\n",
      "[Pre-training #04] loss = 0.2453, AUC/AP: val = 97.28%/97.06%, test = 97.52%/97.\n",
      "[Test] best AUC: val = 97.34%, test = 97.54%\n",
      "[Test] best AP: val = 97.13%, test = 97.27%\n",
      "[Pre-training #05] loss = 0.2380, AUC/AP: val = 97.24%/97.03%, test = 97.43%/97.\n",
      "[Test] best AUC: val = 97.32%, test = 97.48%\n",
      "[Test] best AP: val = 97.10%, test = 97.19%\n",
      "[Pre-training #06] loss = 0.2392, AUC/AP: val = 97.20%/96.96%, test = 97.39%/97.\n",
      "[Test] best AUC: val = 97.28%, test = 97.42%\n",
      "[Test] best AP: val = 97.09%, test = 97.14%\n",
      "[Pre-training #07] loss = 0.2428, AUC/AP: val = 97.09%/96.69%, test = 97.23%/96.\n",
      "[Test] best AUC: val = 97.13%, test = 97.25%\n",
      "[Test] best AP: val = 96.82%, test = 96.80%\n",
      "[Pre-training #08] loss = 0.2378, AUC/AP: val = 97.18%/96.91%, test = 97.41%/97.\n",
      "[Test] best AUC: val = 97.21%, test = 97.41%\n",
      "[Test] best AP: val = 96.94%, test = 97.08%\n",
      "[Pre-training #09] loss = 0.2328, AUC/AP: val = 97.15%/96.85%, test = 97.34%/97.\n",
      "[Test] best AUC: val = 97.23%, test = 97.39%\n",
      "[Test] best AP: val = 96.97%, test = 97.09%\n",
      "[Pre-training #10] loss = 0.2332, AUC/AP: val = 97.22%/96.95%, test = 97.38%/96.\n",
      "[Test] best AUC: val = 97.22%, test = 97.38%\n",
      "[Test] best AP: val = 96.95%, test = 96.99%\n",
      "\n",
      "\n",
      " Final AUC(%): val = 97.23 ± 0.07, test = 97.42 ± 0.08\n",
      " Final AP(%): val = 96.98 ± 0.10, test = 97.09 ± 0.15\n"
     ]
    }
   ],
   "source": [
    "!python train_link.py --dataset=CS --use_cfg --device=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "77a3f638",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Pre-training #01] loss = 0.6916, AUC/AP: val = 96.96%/96.72%, test = 96.87%/96.\n",
      "Training ends by early stopping at ep 480.\n",
      "[Test] best AUC: val = 97.19%, test = 97.06%\n",
      "[Test] best AP: val = 96.86%, test = 96.71%\n",
      "[Pre-training #02] loss = 0.6938, AUC/AP: val = 96.82%/96.51%, test = 96.72%/96.\n",
      "Training ends by early stopping at ep 470.\n",
      "[Test] best AUC: val = 97.15%, test = 97.06%\n",
      "[Test] best AP: val = 96.84%, test = 96.70%\n",
      "[Pre-training #03] loss = 0.6861, AUC/AP: val = 96.85%/96.62%, test = 96.75%/96.\n",
      "Training ends by early stopping at ep 600.\n",
      "[Test] best AUC: val = 97.08%, test = 96.98%\n",
      "[Test] best AP: val = 96.84%, test = 96.68%\n",
      "[Pre-training #04] loss = 0.6932, AUC/AP: val = 96.91%/96.63%, test = 96.79%/96.\n",
      "Training ends by early stopping at ep 540.\n",
      "[Test] best AUC: val = 97.17%, test = 97.05%\n",
      "[Test] best AP: val = 96.89%, test = 96.72%\n",
      "[Pre-training #05] loss = 0.6921, AUC/AP: val = 96.66%/96.40%, test = 96.55%/96.\n",
      "Training ends by early stopping at ep 480.\n",
      "[Test] best AUC: val = 97.03%, test = 96.95%\n",
      "[Test] best AP: val = 96.71%, test = 96.57%\n",
      "[Pre-training #06] loss = 0.6858, AUC/AP: val = 96.93%/96.66%, test = 96.84%/96.\n",
      "Training ends by early stopping at ep 560.\n",
      "[Test] best AUC: val = 97.06%, test = 96.95%\n",
      "[Test] best AP: val = 96.79%, test = 96.62%\n",
      "[Pre-training #07] loss = 0.6898, AUC/AP: val = 96.83%/96.55%, test = 96.75%/96.\n",
      "Training ends by early stopping at ep 750.\n",
      "[Test] best AUC: val = 97.14%, test = 97.02%\n",
      "[Test] best AP: val = 96.91%, test = 96.70%\n",
      "[Pre-training #08] loss = 0.6884, AUC/AP: val = 96.82%/96.53%, test = 96.74%/96.\n",
      "Training ends by early stopping at ep 480.\n",
      "[Test] best AUC: val = 97.13%, test = 97.00%\n",
      "[Test] best AP: val = 96.83%, test = 96.64%\n",
      "[Pre-training #09] loss = 0.6885, AUC/AP: val = 96.73%/96.49%, test = 96.63%/96.\n",
      "Training ends by early stopping at ep 470.\n",
      "[Test] best AUC: val = 97.17%, test = 97.04%\n",
      "[Test] best AP: val = 96.85%, test = 96.66%\n",
      "[Pre-training #10] loss = 0.6883, AUC/AP: val = 97.00%/96.75%, test = 96.90%/96.\n",
      "Training ends by early stopping at ep 610.\n",
      "[Test] best AUC: val = 97.14%, test = 97.05%\n",
      "[Test] best AP: val = 96.86%, test = 96.73%\n",
      "\n",
      "\n",
      " Final AUC(%): val = 97.13 ± 0.05, test = 97.02 ± 0.04\n",
      " Final AP(%): val = 96.84 ± 0.06, test = 96.67 ± 0.05\n"
     ]
    }
   ],
   "source": [
    "!python train_link.py --dataset=Physics --use_cfg --device=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5b6a2835",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:root:The OGB package is out of date. Your version is 1.3.5, while the latest version is 1.3.6.\n",
      "Loading Data...\n",
      "44.02% of edges are dropped according to edge year 2010.\n",
      "[Pre-training #01] loss = 0.0206, Hits@20/Hits@50/Hits@100: val = 99.20%/99.96%/\n",
      "[Test] best Hits@20: val = 99.20%, test = 60.63%\n",
      "[Test] best Hits@50: val = 99.96%, test = 68.43%\n",
      "[Test] best Hits@100: val = 99.99%, test = 71.94%\n",
      "[Pre-training #02] loss = 0.0220, Hits@20/Hits@50/Hits@100: val = 99.29%/99.96%/\n",
      "[Test] best Hits@20: val = 99.29%, test = 60.33%\n",
      "[Test] best Hits@50: val = 99.96%, test = 67.88%\n",
      "[Test] best Hits@100: val = 99.99%, test = 72.08%\n",
      "[Pre-training #03] loss = 0.0199, Hits@20/Hits@50/Hits@100: val = 99.24%/99.93%/\n",
      "[Test] best Hits@20: val = 99.24%, test = 60.41%\n",
      "[Test] best Hits@50: val = 99.93%, test = 67.23%\n",
      "[Test] best Hits@100: val = 99.98%, test = 71.81%\n",
      "[Pre-training #04] loss = 0.0214, Hits@20/Hits@50/Hits@100: val = 99.31%/99.94%/\n",
      "[Test] best Hits@20: val = 99.31%, test = 61.18%\n",
      "[Test] best Hits@50: val = 99.94%, test = 68.16%\n",
      "[Test] best Hits@100: val = 99.99%, test = 72.05%\n",
      "[Pre-training #05] loss = 0.0213, Hits@20/Hits@50/Hits@100: val = 99.44%/99.95%/\n",
      "[Test] best Hits@20: val = 99.44%, test = 60.86%\n",
      "[Test] best Hits@50: val = 99.95%, test = 68.18%\n",
      "[Test] best Hits@100: val = 99.98%, test = 71.43%\n",
      "[Pre-training #06] loss = 0.0237, Hits@20/Hits@50/Hits@100: val = 98.70%/99.94%/\n",
      "[Test] best Hits@20: val = 98.70%, test = 58.63%\n",
      "[Test] best Hits@50: val = 99.94%, test = 67.65%\n",
      "[Test] best Hits@100: val = 99.99%, test = 71.62%\n",
      "[Pre-training #07] loss = 0.0228, Hits@20/Hits@50/Hits@100: val = 99.44%/99.89%/\n",
      "[Test] best Hits@20: val = 99.44%, test = 61.49%\n",
      "[Test] best Hits@50: val = 99.89%, test = 66.11%\n",
      "[Test] best Hits@100: val = 99.98%, test = 71.30%\n",
      "[Pre-training #08] loss = 0.0206, Hits@20/Hits@50/Hits@100: val = 99.23%/99.96%/\n",
      "[Test] best Hits@20: val = 99.23%, test = 60.50%\n",
      "[Test] best Hits@50: val = 99.96%, test = 67.43%\n",
      "[Test] best Hits@100: val = 99.99%, test = 71.66%\n",
      "[Pre-training #09] loss = 0.0220, Hits@20/Hits@50/Hits@100: val = 99.46%/99.96%/\n",
      "[Test] best Hits@20: val = 99.46%, test = 60.81%\n",
      "[Test] best Hits@50: val = 99.96%, test = 67.95%\n",
      "[Test] best Hits@100: val = 99.99%, test = 71.75%\n",
      "[Pre-training #10] loss = 0.0212, Hits@20/Hits@50/Hits@100: val = 99.12%/99.97%/\n",
      "[Test] best Hits@20: val = 99.12%, test = 59.41%\n",
      "[Test] best Hits@50: val = 99.97%, test = 68.64%\n",
      "[Test] best Hits@100: val = 99.99%, test = 72.06%\n",
      "\n",
      "\n",
      " Final Hits@20(%): val = 99.24 ± 0.22, test = 60.42 ± 0.84\n",
      " Final Hits@50(%): val = 99.95 ± 0.02, test = 67.77 ± 0.72\n",
      " Final Hits@100(%): val = 99.99 ± 0.00, test = 71.77 ± 0.27\n"
     ]
    }
   ],
   "source": [
    "!python train_link_ogb.py --use_cfg --device=0"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}